{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part is basically for testing the various classifers and program formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import emotion_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Emotion = emotion_analysis.EmotionAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read audio file 'data\\negative_38.wav', count: 100\n",
      "Read audio file 'data\\neutral_221.wav', count: 200\n",
      "Read audio file 'data\\neutral_490.wav', count: 300\n",
      "Read audio file 'data\\positive_303.wav', count: 400\n",
      "Read audio file 'data\\positive_55.wav', count: 500\n"
     ]
    }
   ],
   "source": [
    "inp, lab = Emotion.prepare_dataset('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy as np\n",
    "dataset = zip(inp, lab)\n",
    "shuffle(dataset)\n",
    "inp, lab = zip(*dataset)\n",
    "#max_value = max([max(i) for i in inp])\n",
    "#normalised_inp = np.asarray(inp)/max_value\n",
    "#inp = np.asarray(inp)/max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Emotion.write_to_csv(inp, lab, 'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest : Better performance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(inp[0:int(len(inp)*3/float(4))],lab[0:int(len(inp)*3/float(4))])\n",
    "#clf = clf.fit(inp, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KNN classifier : Good performance\n",
    "\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#clf = KNeighborsClassifier()\n",
    "#clf = clf.fit(inp[0:int(len(inp)*3/float(4))],lab[0:int(len(inp)*3/float(4))])\n",
    "#clf = clf.fit(inp, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM with RBF : Worst performance\n",
    "\n",
    "#from sklearn import svm\n",
    "#clf = svm.SVC(kernel = 'rbf')\n",
    "#clf = clf.fit(inp[0:int(len(inp)*3/float(4))],lab[0:int(len(inp)*3/float(4))])\n",
    "#clf = clf.fit(inp, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier : Poor performance\n",
    "\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#clf = GaussianNB()\n",
    "#clf = clf.fit(inp[0:int(len(inp)*3/float(4))],lab[0:int(len(inp)*3/float(4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_lab = clf.predict(inp[int(len(inp)*3/float(4)):])\n",
    "act_lab = lab[int(len(inp)*3/float(4)):]\n",
    "#pred_lab = clf.predict(inp)\n",
    "#act_lab = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.581395348837\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "accuracy = np.sum([1 for i in range(len(pred_lab)) if pred_lab[i]==act_lab[i]])/len(pred_lab)\n",
    "print \"Accuracy = \"+str(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = [ 0.51515152  0.59459459  0.61016949]\n",
      "Recall = [ 0.43589744  0.52380952  0.75      ]\n",
      "F1 Score = [ 0.47222222  0.55696203  0.6728972 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(act_lab, pred_lab)\n",
    "print \"Precision = \"+str(precision)+\"\\nRecall = \"+str(recall)+\"\\nF1 Score = \"+str(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual    = Counter({'positive': 48, 'neutral': 42, 'negative': 39})\n",
      "Predicted = Counter({'positive': 59, 'neutral': 37, 'negative': 33})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print \"Actual    = \"+str(collections.Counter(act_lab))\n",
    "print \"Predicted = \"+str(collections.Counter(pred_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.88435374,  0.9       ,  0.88265306]),\n",
       " array([ 0.84415584,  0.87931034,  0.93513514]),\n",
       " array([ 0.86378738,  0.88953488,  0.90813648])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotion.test_classifier(clf, [inp, lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotion.evaluate(clf, 'negative_1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The following part can be used to predict emotion for a recorded voice and a new audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To accept an audio data and predict its emotion\n",
    "import pyaudio\n",
    "import wave\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 10\n",
    "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
    " \n",
    "audio = pyaudio.PyAudio()\n",
    " \n",
    "# start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                rate=RATE, input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "print \"recording...\"\n",
    "frames = []\n",
    " \n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "print \"finished recording\"\n",
    " \n",
    " \n",
    "# stop Recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    " \n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(frames))\n",
    "waveFile.close()\n",
    "\n",
    "import feature_extractor\n",
    "Extractor = feature_extractor.FeatureExtractor()\n",
    "f = Extractor.extract_features('file.wav')\n",
    "\n",
    "plt.plot(wav.read('file.wav')[1])\n",
    "plt.show()\n",
    "\n",
    "clf.predict([f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To read an audio file and predict its emotion \n",
    "import feature_extractor\n",
    "Extractor = feature_extractor.FeatureExtractor()\n",
    "f = Extractor.extract_features('calls\\\\9969957664_TaiyubM_2016-01-07-15-11-09.wav')\n",
    "clf.predict([f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part tests the class written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To test the class written\n",
    "import emotion_analysis\n",
    "emo = emotion_analysis.EmotionAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp, lab = emo.prepare_dataset('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = emo.train_classifier([inp, lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1_score = emo.test_classifier(clf, [inp, lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "emo.evaluate(clf,  audio_signal = wav.read('file.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part is useful for creating custom training dataset, from the recorded Edelweiss sales call. \n",
    "It opens each call and plays it into chunks of 20 sec each and then asks me for a label for those 20 sec of call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import scipy.io.wavfile as wav\n",
    "from __future__ import division\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "segments = []\n",
    "\n",
    "path = \"calls\"\n",
    "c_total = 0\n",
    "c_read = 0\n",
    "c_skipped = 0\n",
    "audio_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "for audio_file in audio_files:\n",
    "    c_total+=1\n",
    "    print \"Reading file: \"+str(c_total)\n",
    "    try:\n",
    "        rate_sig, sig = wav.read(join(path, audio_file))\n",
    "        c_read+=1\n",
    "    except:\n",
    "        c_skipped+=1\n",
    "        continue\n",
    "    for i in range(int(math.ceil(len(sig)/(rate_sig*20)))):\n",
    "        segments.append(sig[i*(rate_sig*20):i*(rate_sig*20) + rate_sig*20])\n",
    "print \"\\n\\n\"\n",
    "print \"Total files: \"+str(c_total)\n",
    "print \"Read files: \"+str(c_read)\n",
    "print \"Skipped files:\"+str(c_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "\n",
    "map_key_to_emotion = {'1':'negative','2':'neutral', '3':'positive'}\n",
    "\n",
    "p = pyaudio.PyAudio()  \n",
    "#open stream  \n",
    "stream = p.open(format = FORMAT,  \n",
    "                channels = CHANNELS,  \n",
    "                rate = rate_sig,  \n",
    "                output = True)  \n",
    "#read data\n",
    "count_label = 0\n",
    "for segment in segments:\n",
    "    count_label+=1\n",
    "    stream.write(segment[0:len(segment)/2])\n",
    "    stream.write(segment[len(segment)/2:])\n",
    "    segment_label = map_key_to_emotion[raw_input()]\n",
    "    \n",
    "    wav.write(\"data\\\\\"+segment_label+\"_\"+str(count_label)+\".wav\", rate_sig, segment)\n",
    "\n",
    "\n",
    "#stop stream  \n",
    "#stream.stop_stream()  \n",
    "#stream.close()  \n",
    "\n",
    "#close PyAudio  \n",
    "#p.terminate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate, data = wav.read('negative_1.wav')\n",
    "stream.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## This part will perform three things for any call:\n",
    "1. Predict the emotion in the entire call.\n",
    "2. Predict the emotion transition between the first half and the second half of the call.\n",
    "3. Predict the emotion in 20 second segments and plot a histogram of emotions or give the count of emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import math\n",
    "file_name = raw_input(\"Enter the name of the call file name.\\n> \")\n",
    "rate_sig, sig = wav.read(file_name)\n",
    "\n",
    "plt.plot(sig)\n",
    "plt.plot((0,len(sig)),(0,0),'g')\n",
    "plt.show()\n",
    "\n",
    "complete_call_result = emo.evaluate(clf, audio_signal = (rate_sig, sig))\n",
    "first_half_result = emo.evaluate(clf, audio_signal = (rate_sig, sig[:int(len(sig)/2)]))\n",
    "second_half_result = emo.evaluate(clf, audio_signal = (rate_sig, sig[int(len(sig)/2):]))\n",
    "segment_result = []\n",
    "for i in range(int(math.ceil(len(sig)/(rate_sig*20)))):\n",
    "    segment_result.append(emo.evaluate(clf, audio_signal = (rate_sig, sig[i*rate_sig*20:i*rate_sig*20+rate_sig*20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "print \"Overall call result: \"+complete_call_result\n",
    "print \"Trainsition: \"+first_half_result+\" --> \"+second_half_result\n",
    "map_dict = {'negative':-1, 'neutral':0, 'positive':1}\n",
    "mapped_segment_result = [map_dict[result] for result in segment_result]\n",
    "index = [i for i in range(len(mapped_segment_result))]\n",
    "index = np.asarray(index)\n",
    "bar_width = 1\n",
    "plt.plot((0,index[-1]+1),((0,0)), 'r')\n",
    "plt.bar(index, mapped_segment_result, bar_width)\n",
    "plt.xticks(index + bar_width/2 , (str(i*20)+'-'+str((i+1)*20) for i in index))\n",
    "plt.xlabel('Time series with 20 seconds chunk')\n",
    "#plt.axes([0, 100, -2, 2])\n",
    "plt.ylabel('Emotion:\\n1 = positive\\n0 = neutral\\n-1 = negative')\n",
    "plt.title('Emotion per 20 second segment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
